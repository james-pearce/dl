{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec is dope. In short, it takes in a corpus, and churns out vectors for each of those words. What's so special about these vectors you ask? Well, similar words are near each other. Furthermore, these vectors represent how we use the words. For example, `v_man - v_woman` is approximately equal to `v_king - v_queen`, illustrating the relationship that \"man is to woman as king is to queen\". This process, in NLP voodoo, is called **word embedding**. These representations have been applied widely. This is made even more awesome with the introduction of Doc2Vec that represents not only words, but entire sentences and documents. Imagine being able to represent an entire sentence using a fixed-length vector and proceeding to run all your standard classification algorithms. Isn't that amazing?\n",
    "\n",
    "However, Word2Vec documentation is shit. The C-code is nigh unreadable (700 lines of highly optimized, and sometimes weirdly optimized code). I personally spent a lot of time untangling Doc2Vec and crashing into ~50% accuracies due to implementation mistakes. This tutorial aims to help other users get off the ground using Word2Vec for their own research. We use Word2Vec for **sentiment analysis** by attempting to classify the Cornell IMDB movie review corpus (http://www.cs.cornell.edu/people/pabo/movie-review-data/).\n",
    "\n",
    "The source code used in this demo can be found at https://github.com/linanqiu/word2vec-sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Modules\n",
    "\n",
    "We use `gensim`, since `gensim` has a much more readable implementation of Word2Vec (and Doc2Vec). Bless those guys. We also use `numpy` for general array manipulation, and `sklearn` for Logistic Regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim modules\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# random\n",
    "import random\n",
    "\n",
    "# os\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Format\n",
    "\n",
    "We can't input the raw reviews from the Cornell movie review data repository. Instead, we clean them up by converting everything to lower case and removing punctuation. I did this via bash, and you can do this easily via Python, JS, or your favorite poison. This step is trivial.\n",
    "\n",
    "The result is to have five documents:\n",
    "\n",
    "- `test-neg.txt`: 12500 negative movie reviews from the test data\n",
    "- `test-pos.txt`: 12500 positive movie reviews from the test data\n",
    "- `train-neg.txt`: 12500 negative movie reviews from the training data\n",
    "- `train-pos.txt`: 12500 positive movie reviews from the training data\n",
    "- `train-unsup.txt`: 50000 Unlabelled movie reviews\n",
    "\n",
    "Each of the reviews should be formatted as such:\n",
    "\n",
    "```\n",
    "once again mr costner has dragged out a movie for far longer than necessary aside from the terrific sea rescue sequences of which there are very few i just did not care about any of the characters most of us have ghosts in the closet and costner s character are realized early on and then forgotten until much later by which time i did not care the character we should really care about is a very cocky overconfident ashton kutcher the problem is he comes off as kid who thinks he s better than anyone else around him and shows no signs of a cluttered closet his only obstacle appears to be winning over costner finally when we are well past the half way point of this stinker costner tells us all about kutcher s ghosts we are told why kutcher is driven to be the best with no prior inkling or foreshadowing no magic here it was all i could do to keep from turning it off an hour in\n",
    "this is an example of why the majority of action films are the same generic and boring there s really nothing worth watching here a complete waste of the then barely tapped talents of ice t and ice cube who ve each proven many times over that they are capable of acting and acting well don t bother with this one go see new jack city ricochet or watch new york undercover for ice t or boyz n the hood higher learning or friday for ice cube and see the real deal ice t s horribly cliched dialogue alone makes this film grate at the teeth and i m still wondering what the heck bill paxton was doing in this film and why the heck does he always play the exact same character from aliens onward every film i ve seen with bill paxton has him playing the exact same irritating character and at least in aliens his character died which made it somewhat gratifying overall this is second rate action trash there are countless better films to see and if you really want to see this one watch judgement night which is practically a carbon copy but has better acting and a better script the only thing that made this at all worth watching was a decent hand on the camera the cinematography was almost refreshing which comes close to making up for the horrible film itself but not quite\n",
    "```\n",
    "\n",
    "The sample up there contains two movie reviews, each one taking up one entire line. Yes, **each document should be on one line, separated by new lines**. This is extremely important, because our parser depends on this to identify sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feeding Data to Doc2Vec\n",
    "\n",
    "Doc2Vec (the portion of `gensim` that implements the Doc2Vec algorithm) does a great job at word embedding, but a terrible job at reading in files. It only takes in `LabeledLineSentence` classes which basically yields `LabeledSentence`, a class from `gensim.models.doc2vec` representing a single sentence. Why the \"Labeled\" word? Well, here's how Doc2Vec differs from Word2Vec.\n",
    "\n",
    "Word2Vec simply converts a word into a vector.\n",
    "\n",
    "Doc2Vec not only does that, but also aggregates all the words in a sentence into a vector. To do that, it simply treats a sentence label as a special word, and does some voodoo on that special word. Hence, that special word is a label for a sentence. \n",
    "\n",
    "So we have to format sentences into\n",
    "\n",
    "```python\n",
    "[['word1', 'word2', 'word3', 'lastword'], ['label1']]\n",
    "```\n",
    "\n",
    "`LabeledSentence` is simply a tidier way to do that. It contains a list of words, and a label for the sentence. We don't really need to care about how `LabeledSentence` works exactly, we just have to know that it stores those two things -- a list of words and a label.\n",
    "\n",
    "However, we need a way to convert our new line separated corpus into a collection of `LabeledSentence`s. The default constructor for the default `LabeledLineSentence` class in Doc2Vec can do that for a single text file, but can't do that for multiple files. In classification tasks however, we usually deal with multiple documents (test, training, positive, negative etc). Ain't that annoying?\n",
    "\n",
    "So we write our own `LabeledLineSentence` class. The constructor takes in a dictionary that defines the files to read and the label prefixes sentences from that document should take on. Then, Doc2Vec can either read the collection directly via the iterator, or we can access the array directly. We also need a function to return a permutated version of the array of `LabeledSentence`s. We'll see why later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "        \n",
    "        flipped = {}\n",
    "        \n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
    "    \n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "    \n",
    "    def sentences_perm(self):\n",
    "        shuffled = list(self.sentences)\n",
    "        random.shuffle(shuffled)\n",
    "        return shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can feed the data files to `LabeledLineSentence`. As we mentioned earlier, `LabeledLineSentence` simply takes a dictionary with keys as the file names and values the special prefixes for sentences from that document. The prefixes need to be unique, so that there is no ambiguitiy for sentences from different documents.\n",
    "\n",
    "The prefixes will have a counter appended to them to label individual sentences in the documetns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = {'test-neg.txt':'TEST_NEG', 'test-pos.txt':'TEST_POS', 'train-neg.txt':'TRAIN_NEG', \n",
    "           'train-pos.txt':'TRAIN_POS', 'train-unsup.txt':'TRAIN_UNS'}\n",
    "\n",
    "sentences = LabeledLineSentence(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = TaggedDocument(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### Building the Vocabulary Table\n",
    "\n",
    "Doc2Vec requires us to build the vocabulary table (simply digesting all the words and filtering out the unique words, and doing some basic counts on them). So we feed it the array of sentences. `model.build_vocab` takes an array of `LabeledLineSentence`, hence our `to_array` function in the `LabeledLineSentences` class. \n",
    "\n",
    "If you're curious about the parameters, do read the Word2Vec documentation. Otherwise, here's a quick rundown:\n",
    "\n",
    "- `min_count`: ignore all words with total frequency lower than this. You have to set this to 1, since the sentence labels only appear once. Setting it any higher than 1 will miss out on the sentences.\n",
    "- `window`: the maximum distance between the current and predicted word within a sentence. Word2Vec uses a skip-gram model, and this is simply the window size of the skip-gram model.\n",
    "- `size`: dimensionality of the feature vectors in output. 100 is a good number. If you're extreme, you can go up to around 400.\n",
    "- `sample`: threshold for configuring which higher-frequency words are randomly downsampled\n",
    "- `workers`: use this many worker threads to train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/jamespearce/repos/dl/data/sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamespearce/miniconda3/envs/dl/lib/python3.6/site-packages/ipykernel_launcher.py:25: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(min_count=1, window=10, vector_size=100, sample=1e-4, negative=5, workers=7,\n",
    "               alpha=0.025, min_alpha=0.025)\n",
    "\n",
    "model.build_vocab(sentences.to_array())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Doc2Vec\n",
    "\n",
    "Now we train the model. The model is better trained if **in each training epoch, the sequence of sentences fed to the model is randomized**. This is important: missing out on this steps gives you really shitty results. This is the reason for the `sentences_perm` method in our `LabeledLineSentences` class.\n",
    "\n",
    "We train it for 10 epochs. If I had more time, I'd have done 20.\n",
    "\n",
    "This process takes around 10 mins, so go grab some coffee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamespearce/miniconda3/envs/dl/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 10\n",
    "for epoch in range(max_epochs):\n",
    "    print ('iteration {0}'.format(epoch))\n",
    "    model.train(sentences.sentences_perm(),\n",
    "               total_examples=model.corpus_count,\n",
    "               epochs=model.iter) # model.epochs ??\n",
    "    model.alpha -= 0.002 # decrease learning rate\n",
    "    model.min_alpha = model.alpha # fix so no decay\n",
    "#     model.train(sentences.sentences_perm())\n",
    "# model.train(sentences.sentences_perm(), total_words=1000, epochs=50) # beware the magic number\n",
    "print ('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the Model\n",
    "\n",
    "Let's see what our model gives. It seems that it has kind of understood the word `good`, since the most similar words to good are `glamorous`, `spectacular`, `astounding` etc. This is really awesome (and important), since we are doing sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nice', 0.6834791302680969),\n",
       " ('decent', 0.6789529323577881),\n",
       " ('great', 0.6568922400474548),\n",
       " ('bad', 0.6568381190299988),\n",
       " ('fine', 0.6295196413993835),\n",
       " ('solid', 0.6145359873771667),\n",
       " ('terrific', 0.5557154417037964),\n",
       " ('fantastic', 0.5430033802986145),\n",
       " ('excellent', 0.5312724709510803),\n",
       " ('disappointing', 0.5216973423957825)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also prop the hood open and see what the model actually contains. This is each of the vectors of the words and sentences in the model. We can access all of them using `model.syn0` (for the geekier ones among you, `syn0` is simply the output layer of the shallow neural network). However, we don't want to use the entire `syn0` since that contains the vectors for the words as well, but we are only interested in the ones for sentences.\n",
    "\n",
    "Here's a sample vector for the first sentence in the training set for negative reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.96430969,  0.10271791,  0.12474386,  0.7186532 ,  1.82451093,\n",
       "        1.2003125 , -0.23583592,  1.01816785,  0.69412196, -0.65088362,\n",
       "        0.57726198, -1.93346131, -1.05376565, -0.26727399, -0.43875602,\n",
       "        0.07049139, -0.91318643,  1.08978844, -0.23533964, -1.03512585,\n",
       "       -1.02525222, -1.13230348, -0.25420204,  0.05271002, -0.74665707,\n",
       "       -0.28303775, -0.50565034,  2.35043812, -0.11464602, -0.29187128,\n",
       "       -1.31907737, -0.42764592,  0.15069836,  1.18162823,  0.16823852,\n",
       "       -0.9416728 ,  0.27668956,  0.40300432, -1.81020391,  0.86010969,\n",
       "        1.19802105, -0.36606476,  1.31520963,  0.57425082, -0.0485524 ,\n",
       "        0.74351275,  1.5635519 ,  1.09844077,  1.85646403,  0.06110585,\n",
       "        0.84082252,  0.7019977 ,  1.02865839, -0.91546881,  0.00917614,\n",
       "       -1.51288021,  1.30717695, -0.01342327,  0.98863602, -1.02998543,\n",
       "       -0.82786661, -0.81280404, -0.26240173,  0.67181259, -0.12052402,\n",
       "       -0.57238591, -0.17024867, -1.30503106, -0.70099688, -0.27943772,\n",
       "       -0.45389396,  1.17962003,  0.55143553, -0.03852808, -1.73405218,\n",
       "       -1.41794646,  0.1609869 , -0.26946506,  0.66398907, -0.84488255,\n",
       "        0.50702494, -0.1243672 ,  0.08282843, -0.4831433 ,  0.8423456 ,\n",
       "        0.56202042,  0.01285527,  1.0148474 , -2.2807076 , -0.05625935,\n",
       "       -0.78336054, -1.68577147, -0.85651517, -1.36471093, -1.46482766,\n",
       "        0.4158597 ,  0.43438217, -0.57779443, -0.31669635,  0.02123069], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['TRAIN_NEG_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading Models\n",
    "\n",
    "To avoid training the model again, we can save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./imdb.d2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load('./imdb.d2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Sentiments\n",
    "\n",
    "### Training Vectors\n",
    "\n",
    "Now let's use these vectors to train a classifier. First, we must extract the training vectors. Remember that we have a total of 25000 training reviews, with equal numbers of positive and negative ones (12500 positive, 12500 negative).\n",
    "\n",
    "Hence, we create a `numpy` array (since the classifier we use only takes numpy arrays. There are two parallel arrays, one containing the vectors (`train_arrays`) and the other containing the labels (`train_labels`).\n",
    "\n",
    "We simply put the positive ones at the first half of the array, and the negative ones at the second half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arrays = np.zeros((25000, 100))\n",
    "train_labels = np.zeros(25000)\n",
    "\n",
    "for i in range(12500):\n",
    "    prefix_train_pos = 'TRAIN_POS_' + str(i)\n",
    "    prefix_train_neg = 'TRAIN_NEG_' + str(i)\n",
    "    train_arrays[i] = model[prefix_train_pos]\n",
    "    train_arrays[12500 + i] = model[prefix_train_neg]\n",
    "    train_labels[i] = 1\n",
    "    train_labels[12500 + i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training array looks like this: rows and rows of vectors representing each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.87297237 -0.38670766 -0.73900181 ...,  0.58850521 -0.68588346\n",
      "  -2.17237258]\n",
      " [-3.88280177 -3.56120467  1.16035223 ...,  2.52555108  0.72337848\n",
      "   0.52997857]\n",
      " [-1.08022726 -1.01178837 -0.47317252 ..., -0.30459636 -0.95116109\n",
      "   0.02349397]\n",
      " ..., \n",
      " [-0.28385359  0.1684605  -0.98514009 ...,  0.66366428 -2.31382298\n",
      "   1.0009079 ]\n",
      " [ 0.27408305 -2.1246891   0.89160365 ...,  2.27301073  0.43184912\n",
      "  -1.51522982]\n",
      " [-0.20670033 -1.0146699   0.37222779 ...,  1.33061361 -0.60825211\n",
      "  -1.41617215]]\n"
     ]
    }
   ],
   "source": [
    "print (train_arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are simply category labels for the sentence vectors -- 1 representing positive and 0 for negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print (train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Vectors\n",
    "\n",
    "We do the same for testing data -- data that we are going to feed to the classifier after we've trained it using the training data. This allows us to evaluate our results. The process is pretty much the same as extracting the results for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arrays = np.zeros((25000, 100))\n",
    "test_labels = np.zeros(25000)\n",
    "\n",
    "for i in range(12500):\n",
    "    prefix_test_pos = 'TEST_POS_' + str(i)\n",
    "    prefix_test_neg = 'TEST_NEG_' + str(i)\n",
    "    test_arrays[i] = model[prefix_test_pos]\n",
    "    test_arrays[12500 + i] = model[prefix_test_neg]\n",
    "    test_labels[i] = 1\n",
    "    test_labels[12500 + i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "Now we train a logistic regression classifier using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_arrays, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And find that we have achieved near 87% accuracy for sentiment analysis. This is rather incredible, given that we are only using a linear SVM and a very shallow neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86643999999999999"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isn't this fantastic? Hope I saved you some time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification in H2O\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"1.8.0_121\"; OpenJDK Runtime Environment (Zulu 8.20.0.5-macosx) (build 1.8.0_121-b15); OpenJDK 64-Bit Server VM (Zulu 8.20.0.5-macosx) (build 25.121-b15, mixed mode)\n",
      "  Starting server from /Users/jamespearce/miniconda3/envs/dl/lib/python3.6/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /var/folders/53/ywlydrqn0dn1x3lz3_c3j0jr0000gn/T/tmpvm4dwjj5\n",
      "  JVM stdout: /var/folders/53/ywlydrqn0dn1x3lz3_c3j0jr0000gn/T/tmpvm4dwjj5/h2o_jamespearce_started_from_python.out\n",
      "  JVM stderr: /var/folders/53/ywlydrqn0dn1x3lz3_c3j0jr0000gn/T/tmpvm4dwjj5/h2o_jamespearce_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>02 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Australia/Melbourne</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.18.0.11</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>2 months and 13 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_jamespearce_bp8ox0</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>7.667 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.6 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------\n",
       "H2O cluster uptime:         02 secs\n",
       "H2O cluster timezone:       Australia/Melbourne\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.18.0.11\n",
       "H2O cluster version age:    2 months and 13 days\n",
       "H2O cluster name:           H2O_from_python_jamespearce_bp8ox0\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    7.667 Gb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.6 final\n",
       "--------------------------  ----------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h2o.init(min_mem_size=\"8G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_0</th>\n",
       "      <th>X_1</th>\n",
       "      <th>X_2</th>\n",
       "      <th>X_3</th>\n",
       "      <th>X_4</th>\n",
       "      <th>X_5</th>\n",
       "      <th>X_6</th>\n",
       "      <th>X_7</th>\n",
       "      <th>X_8</th>\n",
       "      <th>X_9</th>\n",
       "      <th>...</th>\n",
       "      <th>X_91</th>\n",
       "      <th>X_92</th>\n",
       "      <th>X_93</th>\n",
       "      <th>X_94</th>\n",
       "      <th>X_95</th>\n",
       "      <th>X_96</th>\n",
       "      <th>X_97</th>\n",
       "      <th>X_98</th>\n",
       "      <th>X_99</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.872972</td>\n",
       "      <td>-0.386708</td>\n",
       "      <td>-0.739002</td>\n",
       "      <td>-0.211689</td>\n",
       "      <td>-0.692805</td>\n",
       "      <td>1.092025</td>\n",
       "      <td>0.282619</td>\n",
       "      <td>-0.281471</td>\n",
       "      <td>-0.108712</td>\n",
       "      <td>0.531614</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.464429</td>\n",
       "      <td>-1.756734</td>\n",
       "      <td>0.040509</td>\n",
       "      <td>0.615180</td>\n",
       "      <td>-0.018213</td>\n",
       "      <td>1.224909</td>\n",
       "      <td>0.588505</td>\n",
       "      <td>-0.685883</td>\n",
       "      <td>-2.172373</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.882802</td>\n",
       "      <td>-3.561205</td>\n",
       "      <td>1.160352</td>\n",
       "      <td>-1.731776</td>\n",
       "      <td>-1.580566</td>\n",
       "      <td>0.573210</td>\n",
       "      <td>2.434028</td>\n",
       "      <td>0.005852</td>\n",
       "      <td>0.599403</td>\n",
       "      <td>3.115823</td>\n",
       "      <td>...</td>\n",
       "      <td>0.595880</td>\n",
       "      <td>-4.093790</td>\n",
       "      <td>1.911239</td>\n",
       "      <td>-0.129363</td>\n",
       "      <td>-1.769357</td>\n",
       "      <td>-1.461325</td>\n",
       "      <td>2.525551</td>\n",
       "      <td>0.723378</td>\n",
       "      <td>0.529979</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.080227</td>\n",
       "      <td>-1.011788</td>\n",
       "      <td>-0.473173</td>\n",
       "      <td>-0.357528</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>0.701937</td>\n",
       "      <td>0.102466</td>\n",
       "      <td>-1.749127</td>\n",
       "      <td>0.589693</td>\n",
       "      <td>0.094849</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.454196</td>\n",
       "      <td>-0.541502</td>\n",
       "      <td>-0.088640</td>\n",
       "      <td>-0.719358</td>\n",
       "      <td>-0.714584</td>\n",
       "      <td>0.407747</td>\n",
       "      <td>-0.304596</td>\n",
       "      <td>-0.951161</td>\n",
       "      <td>0.023494</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.299750</td>\n",
       "      <td>-0.738922</td>\n",
       "      <td>0.600120</td>\n",
       "      <td>-0.767567</td>\n",
       "      <td>0.517145</td>\n",
       "      <td>-0.035061</td>\n",
       "      <td>0.031394</td>\n",
       "      <td>-0.964378</td>\n",
       "      <td>-1.211100</td>\n",
       "      <td>1.675051</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.160086</td>\n",
       "      <td>1.390050</td>\n",
       "      <td>-0.067820</td>\n",
       "      <td>-1.799572</td>\n",
       "      <td>0.309106</td>\n",
       "      <td>1.030910</td>\n",
       "      <td>1.238531</td>\n",
       "      <td>-1.810848</td>\n",
       "      <td>-1.875456</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.340574</td>\n",
       "      <td>-0.912170</td>\n",
       "      <td>0.293172</td>\n",
       "      <td>0.985274</td>\n",
       "      <td>-0.920305</td>\n",
       "      <td>-0.931942</td>\n",
       "      <td>0.890645</td>\n",
       "      <td>-0.926597</td>\n",
       "      <td>0.388034</td>\n",
       "      <td>1.360730</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.709455</td>\n",
       "      <td>-1.316947</td>\n",
       "      <td>-0.665811</td>\n",
       "      <td>-1.451187</td>\n",
       "      <td>0.912446</td>\n",
       "      <td>-0.637925</td>\n",
       "      <td>0.144428</td>\n",
       "      <td>-2.237141</td>\n",
       "      <td>-1.279103</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        X_0       X_1       X_2       X_3       X_4       X_5       X_6  \\\n",
       "0 -1.872972 -0.386708 -0.739002 -0.211689 -0.692805  1.092025  0.282619   \n",
       "1 -3.882802 -3.561205  1.160352 -1.731776 -1.580566  0.573210  2.434028   \n",
       "2 -1.080227 -1.011788 -0.473173 -0.357528  0.085000  0.701937  0.102466   \n",
       "3 -0.299750 -0.738922  0.600120 -0.767567  0.517145 -0.035061  0.031394   \n",
       "4 -0.340574 -0.912170  0.293172  0.985274 -0.920305 -0.931942  0.890645   \n",
       "\n",
       "        X_7       X_8       X_9  ...        X_91      X_92      X_93  \\\n",
       "0 -0.281471 -0.108712  0.531614  ...   -1.464429 -1.756734  0.040509   \n",
       "1  0.005852  0.599403  3.115823  ...    0.595880 -4.093790  1.911239   \n",
       "2 -1.749127  0.589693  0.094849  ...   -0.454196 -0.541502 -0.088640   \n",
       "3 -0.964378 -1.211100  1.675051  ...   -1.160086  1.390050 -0.067820   \n",
       "4 -0.926597  0.388034  1.360730  ...   -0.709455 -1.316947 -0.665811   \n",
       "\n",
       "       X_94      X_95      X_96      X_97      X_98      X_99  label  \n",
       "0  0.615180 -0.018213  1.224909  0.588505 -0.685883 -2.172373    1.0  \n",
       "1 -0.129363 -1.769357 -1.461325  2.525551  0.723378  0.529979    1.0  \n",
       "2 -0.719358 -0.714584  0.407747 -0.304596 -0.951161  0.023494    1.0  \n",
       "3 -1.799572  0.309106  1.030910  1.238531 -1.810848 -1.875456    1.0  \n",
       "4 -1.451187  0.912446 -0.637925  0.144428 -2.237141 -1.279103    1.0  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame(train_arrays)\n",
    "train_df.columns = [\"X_\" + str(col) for col in train_df.columns]\n",
    "predictors = train_df.columns.tolist()\n",
    "\n",
    "train_df['label'] = train_labels\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_0</th>\n",
       "      <th>X_1</th>\n",
       "      <th>X_2</th>\n",
       "      <th>X_3</th>\n",
       "      <th>X_4</th>\n",
       "      <th>X_5</th>\n",
       "      <th>X_6</th>\n",
       "      <th>X_7</th>\n",
       "      <th>X_8</th>\n",
       "      <th>X_9</th>\n",
       "      <th>...</th>\n",
       "      <th>X_91</th>\n",
       "      <th>X_92</th>\n",
       "      <th>X_93</th>\n",
       "      <th>X_94</th>\n",
       "      <th>X_95</th>\n",
       "      <th>X_96</th>\n",
       "      <th>X_97</th>\n",
       "      <th>X_98</th>\n",
       "      <th>X_99</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.056883</td>\n",
       "      <td>0.257902</td>\n",
       "      <td>0.917862</td>\n",
       "      <td>1.182799</td>\n",
       "      <td>0.011944</td>\n",
       "      <td>-0.427487</td>\n",
       "      <td>0.970976</td>\n",
       "      <td>-0.364005</td>\n",
       "      <td>0.253675</td>\n",
       "      <td>1.067611</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.449020</td>\n",
       "      <td>-0.587609</td>\n",
       "      <td>1.004264</td>\n",
       "      <td>-0.289868</td>\n",
       "      <td>-0.187725</td>\n",
       "      <td>0.497251</td>\n",
       "      <td>0.089193</td>\n",
       "      <td>-0.084107</td>\n",
       "      <td>1.055744</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.451884</td>\n",
       "      <td>-0.487854</td>\n",
       "      <td>2.275770</td>\n",
       "      <td>0.750989</td>\n",
       "      <td>-0.312313</td>\n",
       "      <td>0.352184</td>\n",
       "      <td>0.398389</td>\n",
       "      <td>-1.584866</td>\n",
       "      <td>-0.253115</td>\n",
       "      <td>0.191817</td>\n",
       "      <td>...</td>\n",
       "      <td>1.354279</td>\n",
       "      <td>-0.109212</td>\n",
       "      <td>-3.254560</td>\n",
       "      <td>-2.187411</td>\n",
       "      <td>0.595878</td>\n",
       "      <td>1.386712</td>\n",
       "      <td>-0.833938</td>\n",
       "      <td>1.638248</td>\n",
       "      <td>-0.001585</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.373235</td>\n",
       "      <td>-1.605230</td>\n",
       "      <td>2.652814</td>\n",
       "      <td>1.438553</td>\n",
       "      <td>-1.020367</td>\n",
       "      <td>-1.605608</td>\n",
       "      <td>1.944203</td>\n",
       "      <td>0.136347</td>\n",
       "      <td>2.295100</td>\n",
       "      <td>1.479881</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.611217</td>\n",
       "      <td>0.097585</td>\n",
       "      <td>-3.050295</td>\n",
       "      <td>-1.556332</td>\n",
       "      <td>-1.142373</td>\n",
       "      <td>1.271130</td>\n",
       "      <td>-0.353239</td>\n",
       "      <td>-0.161483</td>\n",
       "      <td>0.550128</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.062646</td>\n",
       "      <td>-1.402895</td>\n",
       "      <td>0.857206</td>\n",
       "      <td>-0.338758</td>\n",
       "      <td>-0.485924</td>\n",
       "      <td>-1.390626</td>\n",
       "      <td>0.936949</td>\n",
       "      <td>-1.338978</td>\n",
       "      <td>0.005824</td>\n",
       "      <td>0.732505</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.579270</td>\n",
       "      <td>0.256764</td>\n",
       "      <td>-0.334779</td>\n",
       "      <td>0.232547</td>\n",
       "      <td>-0.234310</td>\n",
       "      <td>-0.698297</td>\n",
       "      <td>0.526001</td>\n",
       "      <td>-1.230899</td>\n",
       "      <td>-1.205009</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.612610</td>\n",
       "      <td>-1.759970</td>\n",
       "      <td>1.994322</td>\n",
       "      <td>-0.865679</td>\n",
       "      <td>0.791905</td>\n",
       "      <td>2.591355</td>\n",
       "      <td>1.133041</td>\n",
       "      <td>-0.385240</td>\n",
       "      <td>0.303026</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.367788</td>\n",
       "      <td>-0.126217</td>\n",
       "      <td>-2.231446</td>\n",
       "      <td>-0.150784</td>\n",
       "      <td>0.113917</td>\n",
       "      <td>0.346769</td>\n",
       "      <td>-0.623452</td>\n",
       "      <td>-0.091713</td>\n",
       "      <td>0.570061</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        X_0       X_1       X_2       X_3       X_4       X_5       X_6  \\\n",
       "0 -0.056883  0.257902  0.917862  1.182799  0.011944 -0.427487  0.970976   \n",
       "1 -0.451884 -0.487854  2.275770  0.750989 -0.312313  0.352184  0.398389   \n",
       "2  0.373235 -1.605230  2.652814  1.438553 -1.020367 -1.605608  1.944203   \n",
       "3 -1.062646 -1.402895  0.857206 -0.338758 -0.485924 -1.390626  0.936949   \n",
       "4  1.612610 -1.759970  1.994322 -0.865679  0.791905  2.591355  1.133041   \n",
       "\n",
       "        X_7       X_8       X_9  ...        X_91      X_92      X_93  \\\n",
       "0 -0.364005  0.253675  1.067611  ...   -0.449020 -0.587609  1.004264   \n",
       "1 -1.584866 -0.253115  0.191817  ...    1.354279 -0.109212 -3.254560   \n",
       "2  0.136347  2.295100  1.479881  ...   -0.611217  0.097585 -3.050295   \n",
       "3 -1.338978  0.005824  0.732505  ...   -0.579270  0.256764 -0.334779   \n",
       "4 -0.385240  0.303026  0.010593  ...   -1.367788 -0.126217 -2.231446   \n",
       "\n",
       "       X_94      X_95      X_96      X_97      X_98      X_99  label  \n",
       "0 -0.289868 -0.187725  0.497251  0.089193 -0.084107  1.055744    1.0  \n",
       "1 -2.187411  0.595878  1.386712 -0.833938  1.638248 -0.001585    1.0  \n",
       "2 -1.556332 -1.142373  1.271130 -0.353239 -0.161483  0.550128    1.0  \n",
       "3  0.232547 -0.234310 -0.698297  0.526001 -1.230899 -1.205009    1.0  \n",
       "4 -0.150784  0.113917  0.346769 -0.623452 -0.091713  0.570061    1.0  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame(test_arrays)\n",
    "test_df.columns = [\"X_\" + str(col) for col in test_df.columns]\n",
    "predictors = test_df.columns.tolist()\n",
    "\n",
    "test_df['label'] = test_labels\n",
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamespearce/miniconda3/envs/dl/lib/python3.6/site-packages/h2o/utils/shared_utils.py:170: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  data = _handle_python_lists(python_obj.as_matrix().tolist(), -1)[1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "train_h2o = h2o.H2OFrame(train_df)\n",
    "test_h2o = h2o.H2OFrame(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = 'label'\n",
    "\n",
    "# For binary classification, response should be a factor\n",
    "train_h2o[Y] = train_h2o[Y].asfactor()\n",
    "test_h2o[Y] = test_h2o[Y].asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML progress: |████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# Run AutoML for some minutes\n",
    "minutes = 5\n",
    "aml = H2OAutoML(max_runtime_secs = 5*60, \n",
    "#                 exclude_algos=[\"GBM\", \"DRF\", \"DeepLearning\"], \n",
    "                seed=2055)\n",
    "aml.train(x = predictors, y = Y,\n",
    "          training_frame = train_h2o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>model_id                                             </th><th style=\"text-align: right;\">     auc</th><th style=\"text-align: right;\">  logloss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>StackedEnsemble_AllModels_0_AutoML_20180806_222557   </td><td style=\"text-align: right;\">0.936507</td><td style=\"text-align: right;\"> 0.325168</td></tr>\n",
       "<tr><td>StackedEnsemble_BestOfFamily_0_AutoML_20180806_222557</td><td style=\"text-align: right;\">0.936152</td><td style=\"text-align: right;\"> 0.325734</td></tr>\n",
       "<tr><td>GLM_grid_0_AutoML_20180806_222557_model_0            </td><td style=\"text-align: right;\">0.933265</td><td style=\"text-align: right;\"> 0.333024</td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20180806_222557_model_9            </td><td style=\"text-align: right;\">0.927504</td><td style=\"text-align: right;\"> 0.343677</td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20180806_222557_model_15           </td><td style=\"text-align: right;\">0.927502</td><td style=\"text-align: right;\"> 0.344424</td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20180806_222557_model_4            </td><td style=\"text-align: right;\">0.925291</td><td style=\"text-align: right;\"> 0.348505</td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20180806_222557_model_0            </td><td style=\"text-align: right;\">0.924301</td><td style=\"text-align: right;\"> 0.350655</td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20180806_222557_model_1            </td><td style=\"text-align: right;\">0.923835</td><td style=\"text-align: right;\"> 0.351459</td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20180806_222557_model_2            </td><td style=\"text-align: right;\">0.92263 </td><td style=\"text-align: right;\"> 0.35538 </td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20180806_222557_model_7            </td><td style=\"text-align: right;\">0.921052</td><td style=\"text-align: right;\"> 0.431395</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the leaderboard\n",
    "lb = aml.leaderboard\n",
    "lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OStackedEnsembleEstimator :  Stacked Ensemble\n",
      "Model Key:  StackedEnsemble_AllModels_0_AutoML_20180806_222557\n",
      "No model summary for this model\n",
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.03770424172684333\n",
      "RMSE: 0.19417580108459273\n",
      "LogLoss: 0.146354704467591\n",
      "Null degrees of freedom: 20067\n",
      "Residual degrees of freedom: 20055\n",
      "Null deviance: 27819.786690884554\n",
      "Residual deviance: 5874.092418511233\n",
      "AIC: 5900.092418511233\n",
      "AUC: 0.9927791119976621\n",
      "Gini: 0.9855582239953242\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.4130220812610122: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>9366.0</td>\n",
       "<td>711.0</td>\n",
       "<td>0.0706</td>\n",
       "<td> (711.0/10077.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>346.0</td>\n",
       "<td>9645.0</td>\n",
       "<td>0.0346</td>\n",
       "<td> (346.0/9991.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>9712.0</td>\n",
       "<td>10356.0</td>\n",
       "<td>0.0527</td>\n",
       "<td> (1057.0/20068.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0     1      Error    Rate\n",
       "-----  ----  -----  -------  ----------------\n",
       "0      9366  711    0.0706   (711.0/10077.0)\n",
       "1      346   9645   0.0346   (346.0/9991.0)\n",
       "Total  9712  10356  0.0527   (1057.0/20068.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.4130221</td>\n",
       "<td>0.9480513</td>\n",
       "<td>228.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.2479523</td>\n",
       "<td>0.9686033</td>\n",
       "<td>280.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.7182623</td>\n",
       "<td>0.9637949</td>\n",
       "<td>135.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4688593</td>\n",
       "<td>0.9478772</td>\n",
       "<td>211.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9777394</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.1175536</td>\n",
       "<td>1.0</td>\n",
       "<td>328.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9777394</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.4688593</td>\n",
       "<td>0.8958033</td>\n",
       "<td>211.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.4943596</td>\n",
       "<td>0.9470080</td>\n",
       "<td>204.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.4688593</td>\n",
       "<td>0.9478980</td>\n",
       "<td>211.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.413022     0.948051  228\n",
       "max f2                       0.247952     0.968603  280\n",
       "max f0point5                 0.718262     0.963795  135\n",
       "max accuracy                 0.468859     0.947877  211\n",
       "max precision                0.977739     1         0\n",
       "max recall                   0.117554     1         328\n",
       "max specificity              0.977739     1         0\n",
       "max absolute_mcc             0.468859     0.895803  211\n",
       "max min_per_class_accuracy   0.49436      0.947008  204\n",
       "max mean_per_class_accuracy  0.468859     0.947898  211"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 49.79 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100159</td>\n",
       "<td>0.9765526</td>\n",
       "<td>2.0086077</td>\n",
       "<td>2.0086077</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0201181</td>\n",
       "<td>0.0201181</td>\n",
       "<td>100.8607747</td>\n",
       "<td>100.8607747</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200319</td>\n",
       "<td>0.9757117</td>\n",
       "<td>2.0086077</td>\n",
       "<td>2.0086077</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0201181</td>\n",
       "<td>0.0402362</td>\n",
       "<td>100.8607747</td>\n",
       "<td>100.8607747</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300478</td>\n",
       "<td>0.9750917</td>\n",
       "<td>2.0086077</td>\n",
       "<td>2.0086077</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0201181</td>\n",
       "<td>0.0603543</td>\n",
       "<td>100.8607747</td>\n",
       "<td>100.8607747</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400140</td>\n",
       "<td>0.9744964</td>\n",
       "<td>2.0086077</td>\n",
       "<td>2.0086077</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0200180</td>\n",
       "<td>0.0803723</td>\n",
       "<td>100.8607747</td>\n",
       "<td>100.8607747</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500299</td>\n",
       "<td>0.9739236</td>\n",
       "<td>2.0086077</td>\n",
       "<td>2.0086077</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0201181</td>\n",
       "<td>0.1004904</td>\n",
       "<td>100.8607747</td>\n",
       "<td>100.8607747</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000100</td>\n",
       "<td>0.9709830</td>\n",
       "<td>2.0086077</td>\n",
       "<td>2.0086077</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1003904</td>\n",
       "<td>0.2008808</td>\n",
       "<td>100.8607747</td>\n",
       "<td>100.8607747</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500399</td>\n",
       "<td>0.9674312</td>\n",
       "<td>2.0086077</td>\n",
       "<td>2.0086077</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1004904</td>\n",
       "<td>0.3013712</td>\n",
       "<td>100.8607747</td>\n",
       "<td>100.8607747</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000199</td>\n",
       "<td>0.9623489</td>\n",
       "<td>2.0086077</td>\n",
       "<td>2.0086077</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1003904</td>\n",
       "<td>0.4017616</td>\n",
       "<td>100.8607747</td>\n",
       "<td>100.8607747</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000299</td>\n",
       "<td>0.9383943</td>\n",
       "<td>2.0086077</td>\n",
       "<td>2.0086077</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.2008808</td>\n",
       "<td>0.6026424</td>\n",
       "<td>100.8607747</td>\n",
       "<td>100.8607747</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.3999900</td>\n",
       "<td>0.8469254</td>\n",
       "<td>2.0005973</td>\n",
       "<td>2.0066059</td>\n",
       "<td>0.9960120</td>\n",
       "<td>0.9990034</td>\n",
       "<td>0.1999800</td>\n",
       "<td>0.8026224</td>\n",
       "<td>100.0597347</td>\n",
       "<td>100.6605895</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.4837969</td>\n",
       "<td>1.4641720</td>\n",
       "<td>1.8981083</td>\n",
       "<td>0.7289487</td>\n",
       "<td>0.9449870</td>\n",
       "<td>0.1464318</td>\n",
       "<td>0.9490541</td>\n",
       "<td>46.4171965</td>\n",
       "<td>89.8108297</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6000100</td>\n",
       "<td>0.1372613</td>\n",
       "<td>0.5054045</td>\n",
       "<td>1.6659717</td>\n",
       "<td>0.2516193</td>\n",
       "<td>0.8294162</td>\n",
       "<td>0.0505455</td>\n",
       "<td>0.9995996</td>\n",
       "<td>-49.4595460</td>\n",
       "<td>66.5971727</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999701</td>\n",
       "<td>0.0570555</td>\n",
       "<td>0.0040052</td>\n",
       "<td>1.4286324</td>\n",
       "<td>0.0019940</td>\n",
       "<td>0.7112551</td>\n",
       "<td>0.0004004</td>\n",
       "<td>1.0</td>\n",
       "<td>-99.5994800</td>\n",
       "<td>42.8632448</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999801</td>\n",
       "<td>0.0359951</td>\n",
       "<td>0.0</td>\n",
       "<td>1.2500311</td>\n",
       "<td>0.0</td>\n",
       "<td>0.6223371</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>25.0031145</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999900</td>\n",
       "<td>0.0281205</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1111234</td>\n",
       "<td>0.0</td>\n",
       "<td>0.5531809</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>11.1123415</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0209632</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.4978573</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100159                   0.976553           2.00861    2.00861            1                1                           0.0201181       0.0201181                  100.861   100.861\n",
       "    2        0.0200319                   0.975712           2.00861    2.00861            1                1                           0.0201181       0.0402362                  100.861   100.861\n",
       "    3        0.0300478                   0.975092           2.00861    2.00861            1                1                           0.0201181       0.0603543                  100.861   100.861\n",
       "    4        0.040014                    0.974496           2.00861    2.00861            1                1                           0.020018        0.0803723                  100.861   100.861\n",
       "    5        0.0500299                   0.973924           2.00861    2.00861            1                1                           0.0201181       0.10049                    100.861   100.861\n",
       "    6        0.10001                     0.970983           2.00861    2.00861            1                1                           0.10039         0.200881                   100.861   100.861\n",
       "    7        0.15004                     0.967431           2.00861    2.00861            1                1                           0.10049         0.301371                   100.861   100.861\n",
       "    8        0.20002                     0.962349           2.00861    2.00861            1                1                           0.10039         0.401762                   100.861   100.861\n",
       "    9        0.30003                     0.938394           2.00861    2.00861            1                1                           0.200881        0.602642                   100.861   100.861\n",
       "    10       0.39999                     0.846925           2.0006     2.00661            0.996012         0.999003                    0.19998         0.802622                   100.06    100.661\n",
       "    11       0.5                         0.483797           1.46417    1.89811            0.728949         0.944987                    0.146432        0.949054                   46.4172   89.8108\n",
       "    12       0.60001                     0.137261           0.505405   1.66597            0.251619         0.829416                    0.0505455       0.9996                     -49.4595  66.5972\n",
       "    13       0.69997                     0.0570555          0.0040052  1.42863            0.00199402       0.711255                    0.00040036      1                          -99.5995  42.8632\n",
       "    14       0.79998                     0.0359951          0          1.25003            0                0.622337                    0               1                          -100      25.0031\n",
       "    15       0.89999                     0.0281205          0          1.11112            0                0.553181                    0               1                          -100      11.1123\n",
       "    16       1                           0.0209632          0          1                  0                0.497857                    0               1                          -100      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.10160634528899569\n",
      "RMSE: 0.31875750232582084\n",
      "LogLoss: 0.33141342582651173\n",
      "Null degrees of freedom: 4931\n",
      "Residual degrees of freedom: 4919\n",
      "Null deviance: 6838.031463986159\n",
      "Residual deviance: 3269.062032352712\n",
      "AIC: 3295.062032352712\n",
      "AUC: 0.9342239172984684\n",
      "Gini: 0.8684478345969369\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.49994828185931894: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>2067.0</td>\n",
       "<td>356.0</td>\n",
       "<td>0.1469</td>\n",
       "<td> (356.0/2423.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>318.0</td>\n",
       "<td>2191.0</td>\n",
       "<td>0.1267</td>\n",
       "<td> (318.0/2509.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>2385.0</td>\n",
       "<td>2547.0</td>\n",
       "<td>0.1367</td>\n",
       "<td> (674.0/4932.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0     1     Error    Rate\n",
       "-----  ----  ----  -------  --------------\n",
       "0      2067  356   0.1469   (356.0/2423.0)\n",
       "1      318   2191  0.1267   (318.0/2509.0)\n",
       "Total  2385  2547  0.1367   (674.0/4932.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.4999483</td>\n",
       "<td>0.8666930</td>\n",
       "<td>205.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1567613</td>\n",
       "<td>0.9064151</td>\n",
       "<td>313.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.7683774</td>\n",
       "<td>0.8815899</td>\n",
       "<td>116.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4999483</td>\n",
       "<td>0.8633414</td>\n",
       "<td>205.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9766120</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0342643</td>\n",
       "<td>1.0</td>\n",
       "<td>389.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9766120</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.4999483</td>\n",
       "<td>0.7266126</td>\n",
       "<td>205.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.5360029</td>\n",
       "<td>0.8617416</td>\n",
       "<td>193.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.5505126</td>\n",
       "<td>0.8632279</td>\n",
       "<td>188.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.499948     0.866693  205\n",
       "max f2                       0.156761     0.906415  313\n",
       "max f0point5                 0.768377     0.88159   116\n",
       "max accuracy                 0.499948     0.863341  205\n",
       "max precision                0.976612     1         0\n",
       "max recall                   0.0342643    1         389\n",
       "max specificity              0.976612     1         0\n",
       "max absolute_mcc             0.499948     0.726613  205\n",
       "max min_per_class_accuracy   0.536003     0.861742  193\n",
       "max mean_per_class_accuracy  0.550513     0.863228  188"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 50.87 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0101379</td>\n",
       "<td>0.9726630</td>\n",
       "<td>1.9657234</td>\n",
       "<td>1.9657234</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0199283</td>\n",
       "<td>0.0199283</td>\n",
       "<td>96.5723396</td>\n",
       "<td>96.5723396</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200730</td>\n",
       "<td>0.9711930</td>\n",
       "<td>1.9256066</td>\n",
       "<td>1.9458676</td>\n",
       "<td>0.9795918</td>\n",
       "<td>0.9898990</td>\n",
       "<td>0.0191311</td>\n",
       "<td>0.0390594</td>\n",
       "<td>92.5606592</td>\n",
       "<td>94.5867604</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300081</td>\n",
       "<td>0.9699010</td>\n",
       "<td>1.9657234</td>\n",
       "<td>1.9524415</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9932432</td>\n",
       "<td>0.0195297</td>\n",
       "<td>0.0585891</td>\n",
       "<td>96.5723396</td>\n",
       "<td>95.2441481</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0401460</td>\n",
       "<td>0.9686289</td>\n",
       "<td>1.9264089</td>\n",
       "<td>1.9458676</td>\n",
       "<td>0.98</td>\n",
       "<td>0.9898990</td>\n",
       "<td>0.0195297</td>\n",
       "<td>0.0781188</td>\n",
       "<td>92.6408928</td>\n",
       "<td>94.5867604</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500811</td>\n",
       "<td>0.9670187</td>\n",
       "<td>1.9256066</td>\n",
       "<td>1.9418482</td>\n",
       "<td>0.9795918</td>\n",
       "<td>0.9878543</td>\n",
       "<td>0.0191311</td>\n",
       "<td>0.0972499</td>\n",
       "<td>92.5606592</td>\n",
       "<td>94.1848213</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1001622</td>\n",
       "<td>0.9612030</td>\n",
       "<td>1.9179730</td>\n",
       "<td>1.9299106</td>\n",
       "<td>0.9757085</td>\n",
       "<td>0.9817814</td>\n",
       "<td>0.0960542</td>\n",
       "<td>0.1933041</td>\n",
       "<td>91.7973030</td>\n",
       "<td>92.9910621</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500406</td>\n",
       "<td>0.9537619</td>\n",
       "<td>1.9497419</td>\n",
       "<td>1.9365032</td>\n",
       "<td>0.9918699</td>\n",
       "<td>0.9851351</td>\n",
       "<td>0.0972499</td>\n",
       "<td>0.2905540</td>\n",
       "<td>94.9741905</td>\n",
       "<td>93.6503183</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2001217</td>\n",
       "<td>0.9438967</td>\n",
       "<td>1.8940978</td>\n",
       "<td>1.9258911</td>\n",
       "<td>0.9635628</td>\n",
       "<td>0.9797366</td>\n",
       "<td>0.0948585</td>\n",
       "<td>0.3854125</td>\n",
       "<td>89.4097847</td>\n",
       "<td>92.5891108</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000811</td>\n",
       "<td>0.9043114</td>\n",
       "<td>1.7743345</td>\n",
       "<td>1.8754064</td>\n",
       "<td>0.9026369</td>\n",
       "<td>0.9540541</td>\n",
       "<td>0.1773615</td>\n",
       "<td>0.5627740</td>\n",
       "<td>77.4334505</td>\n",
       "<td>87.5406375</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000406</td>\n",
       "<td>0.8045454</td>\n",
       "<td>1.6706655</td>\n",
       "<td>1.8242471</td>\n",
       "<td>0.8498986</td>\n",
       "<td>0.9280284</td>\n",
       "<td>0.1669988</td>\n",
       "<td>0.7297728</td>\n",
       "<td>67.0665523</td>\n",
       "<td>82.4247105</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.5543181</td>\n",
       "<td>1.2599769</td>\n",
       "<td>1.7114388</td>\n",
       "<td>0.6409736</td>\n",
       "<td>0.8706407</td>\n",
       "<td>0.1259466</td>\n",
       "<td>0.8557194</td>\n",
       "<td>25.9976862</td>\n",
       "<td>71.1438820</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999594</td>\n",
       "<td>0.2428213</td>\n",
       "<td>0.7615683</td>\n",
       "<td>1.5531806</td>\n",
       "<td>0.3874239</td>\n",
       "<td>0.7901318</td>\n",
       "<td>0.0761259</td>\n",
       "<td>0.9318454</td>\n",
       "<td>-23.8431707</td>\n",
       "<td>55.3180568</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999189</td>\n",
       "<td>0.1044181</td>\n",
       "<td>0.3867650</td>\n",
       "<td>1.3865981</td>\n",
       "<td>0.1967546</td>\n",
       "<td>0.7053882</td>\n",
       "<td>0.0386608</td>\n",
       "<td>0.9705062</td>\n",
       "<td>-61.3234951</td>\n",
       "<td>38.6598050</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7998783</td>\n",
       "<td>0.0558080</td>\n",
       "<td>0.1913889</td>\n",
       "<td>1.2372348</td>\n",
       "<td>0.0973631</td>\n",
       "<td>0.6294043</td>\n",
       "<td>0.0191311</td>\n",
       "<td>0.9896373</td>\n",
       "<td>-80.8611110</td>\n",
       "<td>23.7234776</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8998378</td>\n",
       "<td>0.0374993</td>\n",
       "<td>0.0837326</td>\n",
       "<td>1.1090968</td>\n",
       "<td>0.0425963</td>\n",
       "<td>0.5642181</td>\n",
       "<td>0.0083699</td>\n",
       "<td>0.9980072</td>\n",
       "<td>-91.6267360</td>\n",
       "<td>10.9096751</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0219362</td>\n",
       "<td>0.0198960</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0101215</td>\n",
       "<td>0.5087186</td>\n",
       "<td>0.0019928</td>\n",
       "<td>1.0</td>\n",
       "<td>-98.0104014</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0101379                   0.972663           1.96572    1.96572            1                1                           0.0199283       0.0199283                  96.5723   96.5723\n",
       "    2        0.020073                    0.971193           1.92561    1.94587            0.979592         0.989899                    0.0191311       0.0390594                  92.5607   94.5868\n",
       "    3        0.0300081                   0.969901           1.96572    1.95244            1                0.993243                    0.0195297       0.0585891                  96.5723   95.2441\n",
       "    4        0.040146                    0.968629           1.92641    1.94587            0.98             0.989899                    0.0195297       0.0781188                  92.6409   94.5868\n",
       "    5        0.0500811                   0.967019           1.92561    1.94185            0.979592         0.987854                    0.0191311       0.0972499                  92.5607   94.1848\n",
       "    6        0.100162                    0.961203           1.91797    1.92991            0.975709         0.981781                    0.0960542       0.193304                   91.7973   92.9911\n",
       "    7        0.150041                    0.953762           1.94974    1.9365             0.99187          0.985135                    0.0972499       0.290554                   94.9742   93.6503\n",
       "    8        0.200122                    0.943897           1.8941     1.92589            0.963563         0.979737                    0.0948585       0.385413                   89.4098   92.5891\n",
       "    9        0.300081                    0.904311           1.77433    1.87541            0.902637         0.954054                    0.177361        0.562774                   77.4335   87.5406\n",
       "    10       0.400041                    0.804545           1.67067    1.82425            0.849899         0.928028                    0.166999        0.729773                   67.0666   82.4247\n",
       "    11       0.5                         0.554318           1.25998    1.71144            0.640974         0.870641                    0.125947        0.855719                   25.9977   71.1439\n",
       "    12       0.599959                    0.242821           0.761568   1.55318            0.387424         0.790132                    0.0761259       0.931845                   -23.8432  55.3181\n",
       "    13       0.699919                    0.104418           0.386765   1.3866             0.196755         0.705388                    0.0386608       0.970506                   -61.3235  38.6598\n",
       "    14       0.799878                    0.055808           0.191389   1.23723            0.0973631        0.629404                    0.0191311       0.989637                   -80.8611  23.7235\n",
       "    15       0.899838                    0.0374993          0.0837326  1.1091             0.0425963        0.564218                    0.00836987      0.998007                   -91.6267  10.9097\n",
       "    16       1                           0.0219362          0.019896   1                  0.0101215        0.508719                    0.00199283      1                          -98.0104  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on cross-validation data. **\n",
      "\n",
      "MSE: 0.09917463355076918\n",
      "RMSE: 0.31492004310740396\n",
      "LogLoss: 0.32516766415604176\n",
      "Null degrees of freedom: 20067\n",
      "Residual degrees of freedom: 20053\n",
      "Null deviance: 27828.826038623632\n",
      "Residual deviance: 13050.929368566893\n",
      "AIC: 13080.929368566893\n",
      "AUC: 0.9365071017026368\n",
      "Gini: 0.8730142034052737\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.45716668609840594: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>8606.0</td>\n",
       "<td>1471.0</td>\n",
       "<td>0.146</td>\n",
       "<td> (1471.0/10077.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>1227.0</td>\n",
       "<td>8764.0</td>\n",
       "<td>0.1228</td>\n",
       "<td> (1227.0/9991.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>9833.0</td>\n",
       "<td>10235.0</td>\n",
       "<td>0.1344</td>\n",
       "<td> (2698.0/20068.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0     1      Error    Rate\n",
       "-----  ----  -----  -------  ----------------\n",
       "0      8606  1471   0.146    (1471.0/10077.0)\n",
       "1      1227  8764   0.1228   (1227.0/9991.0)\n",
       "Total  9833  10235  0.1344   (2698.0/20068.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.4571667</td>\n",
       "<td>0.8666073</td>\n",
       "<td>214.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1266258</td>\n",
       "<td>0.9055332</td>\n",
       "<td>330.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.7393757</td>\n",
       "<td>0.8797216</td>\n",
       "<td>126.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4571667</td>\n",
       "<td>0.8655571</td>\n",
       "<td>214.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9762402</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0280349</td>\n",
       "<td>1.0</td>\n",
       "<td>396.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9762402</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.4571667</td>\n",
       "<td>0.7313535</td>\n",
       "<td>214.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.4983875</td>\n",
       "<td>0.8643779</td>\n",
       "<td>202.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.4571667</td>\n",
       "<td>0.8656067</td>\n",
       "<td>214.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.457167     0.866607  214\n",
       "max f2                       0.126626     0.905533  330\n",
       "max f0point5                 0.739376     0.879722  126\n",
       "max accuracy                 0.457167     0.865557  214\n",
       "max precision                0.97624      1         0\n",
       "max recall                   0.0280349    1         396\n",
       "max specificity              0.97624      1         0\n",
       "max absolute_mcc             0.457167     0.731354  214\n",
       "max min_per_class_accuracy   0.498387     0.864378  202\n",
       "max mean_per_class_accuracy  0.457167     0.865607  214"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 49.79 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100159</td>\n",
       "<td>0.9727256</td>\n",
       "<td>2.0086077</td>\n",
       "<td>2.0086077</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0201181</td>\n",
       "<td>0.0201181</td>\n",
       "<td>100.8607747</td>\n",
       "<td>100.8607747</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200319</td>\n",
       "<td>0.9710852</td>\n",
       "<td>2.0086077</td>\n",
       "<td>2.0086077</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0201181</td>\n",
       "<td>0.0402362</td>\n",
       "<td>100.8607747</td>\n",
       "<td>100.8607747</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300478</td>\n",
       "<td>0.9695620</td>\n",
       "<td>1.9786285</td>\n",
       "<td>1.9986147</td>\n",
       "<td>0.9850746</td>\n",
       "<td>0.9950249</td>\n",
       "<td>0.0198178</td>\n",
       "<td>0.0600540</td>\n",
       "<td>97.8628527</td>\n",
       "<td>99.8614674</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400140</td>\n",
       "<td>0.9683753</td>\n",
       "<td>1.9784786</td>\n",
       "<td>1.9935995</td>\n",
       "<td>0.985</td>\n",
       "<td>0.9925280</td>\n",
       "<td>0.0197177</td>\n",
       "<td>0.0797718</td>\n",
       "<td>97.8478631</td>\n",
       "<td>99.3599470</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500299</td>\n",
       "<td>0.9671935</td>\n",
       "<td>1.9586424</td>\n",
       "<td>1.9866011</td>\n",
       "<td>0.9751244</td>\n",
       "<td>0.9890438</td>\n",
       "<td>0.0196177</td>\n",
       "<td>0.0993895</td>\n",
       "<td>95.8642380</td>\n",
       "<td>98.6601088</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000100</td>\n",
       "<td>0.9612347</td>\n",
       "<td>1.9725609</td>\n",
       "<td>1.9795845</td>\n",
       "<td>0.9820538</td>\n",
       "<td>0.9855506</td>\n",
       "<td>0.0985887</td>\n",
       "<td>0.1979782</td>\n",
       "<td>97.2560948</td>\n",
       "<td>97.9584516</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500399</td>\n",
       "<td>0.9539609</td>\n",
       "<td>1.9505902</td>\n",
       "<td>1.9699165</td>\n",
       "<td>0.9711155</td>\n",
       "<td>0.9807373</td>\n",
       "<td>0.0975878</td>\n",
       "<td>0.2955660</td>\n",
       "<td>95.0590193</td>\n",
       "<td>96.9916532</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000199</td>\n",
       "<td>0.9436005</td>\n",
       "<td>1.9244985</td>\n",
       "<td>1.9585677</td>\n",
       "<td>0.9581256</td>\n",
       "<td>0.9750872</td>\n",
       "<td>0.0961866</td>\n",
       "<td>0.3917526</td>\n",
       "<td>92.4498549</td>\n",
       "<td>95.8567693</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000299</td>\n",
       "<td>0.9028300</td>\n",
       "<td>1.8314660</td>\n",
       "<td>1.9162004</td>\n",
       "<td>0.9118087</td>\n",
       "<td>0.9539944</td>\n",
       "<td>0.1831648</td>\n",
       "<td>0.5749174</td>\n",
       "<td>83.1465958</td>\n",
       "<td>91.6200448</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.3999900</td>\n",
       "<td>0.7850233</td>\n",
       "<td>1.6551489</td>\n",
       "<td>1.8509619</td>\n",
       "<td>0.8240279</td>\n",
       "<td>0.9215149</td>\n",
       "<td>0.1654489</td>\n",
       "<td>0.7403663</td>\n",
       "<td>65.5148856</td>\n",
       "<td>85.0961941</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.4906691</td>\n",
       "<td>1.2610093</td>\n",
       "<td>1.7329597</td>\n",
       "<td>0.6278027</td>\n",
       "<td>0.8627666</td>\n",
       "<td>0.1261135</td>\n",
       "<td>0.8664798</td>\n",
       "<td>26.1009348</td>\n",
       "<td>73.2959664</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6000100</td>\n",
       "<td>0.2116627</td>\n",
       "<td>0.7255808</td>\n",
       "<td>1.5650492</td>\n",
       "<td>0.3612357</td>\n",
       "<td>0.7791712</td>\n",
       "<td>0.0725653</td>\n",
       "<td>0.9390451</td>\n",
       "<td>-27.4419224</td>\n",
       "<td>56.5049239</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999701</td>\n",
       "<td>0.0966206</td>\n",
       "<td>0.3654745</td>\n",
       "<td>1.3937424</td>\n",
       "<td>0.1819541</td>\n",
       "<td>0.6938848</td>\n",
       "<td>0.0365329</td>\n",
       "<td>0.9755780</td>\n",
       "<td>-63.4525510</td>\n",
       "<td>39.3742415</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999801</td>\n",
       "<td>0.0548497</td>\n",
       "<td>0.1531226</td>\n",
       "<td>1.2386456</td>\n",
       "<td>0.0762332</td>\n",
       "<td>0.6166687</td>\n",
       "<td>0.0153138</td>\n",
       "<td>0.9908918</td>\n",
       "<td>-84.6877436</td>\n",
       "<td>23.8645614</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999900</td>\n",
       "<td>0.0376259</td>\n",
       "<td>0.0710569</td>\n",
       "<td>1.1088992</td>\n",
       "<td>0.0353762</td>\n",
       "<td>0.5520735</td>\n",
       "<td>0.0071064</td>\n",
       "<td>0.9979982</td>\n",
       "<td>-92.8943124</td>\n",
       "<td>10.8899166</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0203093</td>\n",
       "<td>0.0200160</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0099651</td>\n",
       "<td>0.4978573</td>\n",
       "<td>0.0020018</td>\n",
       "<td>1.0</td>\n",
       "<td>-97.9983979</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100159                   0.972726           2.00861    2.00861            1                1                           0.0201181       0.0201181                  100.861   100.861\n",
       "    2        0.0200319                   0.971085           2.00861    2.00861            1                1                           0.0201181       0.0402362                  100.861   100.861\n",
       "    3        0.0300478                   0.969562           1.97863    1.99861            0.985075         0.995025                    0.0198178       0.060054                   97.8629   99.8615\n",
       "    4        0.040014                    0.968375           1.97848    1.9936             0.985            0.992528                    0.0197177       0.0797718                  97.8479   99.3599\n",
       "    5        0.0500299                   0.967193           1.95864    1.9866             0.975124         0.989044                    0.0196177       0.0993895                  95.8642   98.6601\n",
       "    6        0.10001                     0.961235           1.97256    1.97958            0.982054         0.985551                    0.0985887       0.197978                   97.2561   97.9585\n",
       "    7        0.15004                     0.953961           1.95059    1.96992            0.971116         0.980737                    0.0975878       0.295566                   95.059    96.9917\n",
       "    8        0.20002                     0.9436             1.9245     1.95857            0.958126         0.975087                    0.0961866       0.391753                   92.4499   95.8568\n",
       "    9        0.30003                     0.90283            1.83147    1.9162             0.911809         0.953994                    0.183165        0.574917                   83.1466   91.62\n",
       "    10       0.39999                     0.785023           1.65515    1.85096            0.824028         0.921515                    0.165449        0.740366                   65.5149   85.0962\n",
       "    11       0.5                         0.490669           1.26101    1.73296            0.627803         0.862767                    0.126114        0.86648                    26.1009   73.296\n",
       "    12       0.60001                     0.211663           0.725581   1.56505            0.361236         0.779171                    0.0725653       0.939045                   -27.4419  56.5049\n",
       "    13       0.69997                     0.0966206          0.365474   1.39374            0.181954         0.693885                    0.0365329       0.975578                   -63.4526  39.3742\n",
       "    14       0.79998                     0.0548497          0.153123   1.23865            0.0762332        0.616669                    0.0153138       0.990892                   -84.6877  23.8646\n",
       "    15       0.89999                     0.0376259          0.0710569  1.1089             0.0353762        0.552074                    0.0071064       0.997998                   -92.8943  10.8899\n",
       "    16       1                           0.0203093          0.020016   1                  0.00996512       0.497857                    0.0020018       1                          -97.9984  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stackedensemble prediction progress: |████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# predict against the test set\n",
    "preds = aml.leader.predict(test_h2o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  predict</th><th style=\"text-align: right;\">       p0</th><th style=\"text-align: right;\">      p1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.0638986</td><td style=\"text-align: right;\">0.936101</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.0313767</td><td style=\"text-align: right;\">0.968623</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.0380281</td><td style=\"text-align: right;\">0.961972</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.0315846</td><td style=\"text-align: right;\">0.968415</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.0262308</td><td style=\"text-align: right;\">0.973769</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.0332806</td><td style=\"text-align: right;\">0.966719</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.0611084</td><td style=\"text-align: right;\">0.938892</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.184531 </td><td style=\"text-align: right;\">0.815469</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.242612 </td><td style=\"text-align: right;\">0.757388</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.0420261</td><td style=\"text-align: right;\">0.957974</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_0</th>\n",
       "      <th>X_1</th>\n",
       "      <th>X_2</th>\n",
       "      <th>X_3</th>\n",
       "      <th>X_4</th>\n",
       "      <th>X_5</th>\n",
       "      <th>X_6</th>\n",
       "      <th>X_7</th>\n",
       "      <th>X_8</th>\n",
       "      <th>X_9</th>\n",
       "      <th>...</th>\n",
       "      <th>X_92</th>\n",
       "      <th>X_93</th>\n",
       "      <th>X_94</th>\n",
       "      <th>X_95</th>\n",
       "      <th>X_96</th>\n",
       "      <th>X_97</th>\n",
       "      <th>X_98</th>\n",
       "      <th>X_99</th>\n",
       "      <th>label</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.056883</td>\n",
       "      <td>0.257902</td>\n",
       "      <td>0.917862</td>\n",
       "      <td>1.182799</td>\n",
       "      <td>0.011944</td>\n",
       "      <td>-0.427487</td>\n",
       "      <td>0.970976</td>\n",
       "      <td>-0.364005</td>\n",
       "      <td>0.253675</td>\n",
       "      <td>1.067611</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.587609</td>\n",
       "      <td>1.004264</td>\n",
       "      <td>-0.289868</td>\n",
       "      <td>-0.187725</td>\n",
       "      <td>0.497251</td>\n",
       "      <td>0.089193</td>\n",
       "      <td>-0.084107</td>\n",
       "      <td>1.055744</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.451884</td>\n",
       "      <td>-0.487854</td>\n",
       "      <td>2.275770</td>\n",
       "      <td>0.750989</td>\n",
       "      <td>-0.312313</td>\n",
       "      <td>0.352184</td>\n",
       "      <td>0.398389</td>\n",
       "      <td>-1.584866</td>\n",
       "      <td>-0.253115</td>\n",
       "      <td>0.191817</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109212</td>\n",
       "      <td>-3.254560</td>\n",
       "      <td>-2.187411</td>\n",
       "      <td>0.595878</td>\n",
       "      <td>1.386712</td>\n",
       "      <td>-0.833938</td>\n",
       "      <td>1.638248</td>\n",
       "      <td>-0.001585</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.373235</td>\n",
       "      <td>-1.605230</td>\n",
       "      <td>2.652814</td>\n",
       "      <td>1.438553</td>\n",
       "      <td>-1.020367</td>\n",
       "      <td>-1.605608</td>\n",
       "      <td>1.944203</td>\n",
       "      <td>0.136347</td>\n",
       "      <td>2.295100</td>\n",
       "      <td>1.479881</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097585</td>\n",
       "      <td>-3.050295</td>\n",
       "      <td>-1.556332</td>\n",
       "      <td>-1.142373</td>\n",
       "      <td>1.271130</td>\n",
       "      <td>-0.353239</td>\n",
       "      <td>-0.161483</td>\n",
       "      <td>0.550128</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.062646</td>\n",
       "      <td>-1.402895</td>\n",
       "      <td>0.857206</td>\n",
       "      <td>-0.338758</td>\n",
       "      <td>-0.485924</td>\n",
       "      <td>-1.390626</td>\n",
       "      <td>0.936949</td>\n",
       "      <td>-1.338978</td>\n",
       "      <td>0.005824</td>\n",
       "      <td>0.732505</td>\n",
       "      <td>...</td>\n",
       "      <td>0.256764</td>\n",
       "      <td>-0.334779</td>\n",
       "      <td>0.232547</td>\n",
       "      <td>-0.234310</td>\n",
       "      <td>-0.698297</td>\n",
       "      <td>0.526001</td>\n",
       "      <td>-1.230899</td>\n",
       "      <td>-1.205009</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.612610</td>\n",
       "      <td>-1.759970</td>\n",
       "      <td>1.994322</td>\n",
       "      <td>-0.865679</td>\n",
       "      <td>0.791905</td>\n",
       "      <td>2.591355</td>\n",
       "      <td>1.133041</td>\n",
       "      <td>-0.385240</td>\n",
       "      <td>0.303026</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.126217</td>\n",
       "      <td>-2.231446</td>\n",
       "      <td>-0.150784</td>\n",
       "      <td>0.113917</td>\n",
       "      <td>0.346769</td>\n",
       "      <td>-0.623452</td>\n",
       "      <td>-0.091713</td>\n",
       "      <td>0.570061</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        X_0       X_1       X_2       X_3       X_4       X_5       X_6  \\\n",
       "0 -0.056883  0.257902  0.917862  1.182799  0.011944 -0.427487  0.970976   \n",
       "1 -0.451884 -0.487854  2.275770  0.750989 -0.312313  0.352184  0.398389   \n",
       "2  0.373235 -1.605230  2.652814  1.438553 -1.020367 -1.605608  1.944203   \n",
       "3 -1.062646 -1.402895  0.857206 -0.338758 -0.485924 -1.390626  0.936949   \n",
       "4  1.612610 -1.759970  1.994322 -0.865679  0.791905  2.591355  1.133041   \n",
       "\n",
       "        X_7       X_8       X_9   ...         X_92      X_93      X_94  \\\n",
       "0 -0.364005  0.253675  1.067611   ...    -0.587609  1.004264 -0.289868   \n",
       "1 -1.584866 -0.253115  0.191817   ...    -0.109212 -3.254560 -2.187411   \n",
       "2  0.136347  2.295100  1.479881   ...     0.097585 -3.050295 -1.556332   \n",
       "3 -1.338978  0.005824  0.732505   ...     0.256764 -0.334779  0.232547   \n",
       "4 -0.385240  0.303026  0.010593   ...    -0.126217 -2.231446 -0.150784   \n",
       "\n",
       "       X_95      X_96      X_97      X_98      X_99  label  predict  \n",
       "0 -0.187725  0.497251  0.089193 -0.084107  1.055744    1.0        1  \n",
       "1  0.595878  1.386712 -0.833938  1.638248 -0.001585    1.0        1  \n",
       "2 -1.142373  1.271130 -0.353239 -0.161483  0.550128    1.0        1  \n",
       "3 -0.234310 -0.698297  0.526001 -1.230899 -1.205009    1.0        1  \n",
       "4  0.113917  0.346769 -0.623452 -0.091713  0.570061    1.0        1  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['predict'] = preds['predict'].as_data_frame()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predict</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>10826</td>\n",
       "      <td>1674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>1700</td>\n",
       "      <td>10800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predict      0      1\n",
       "label                \n",
       "0.0      10826   1674\n",
       "1.0       1700  10800"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(test_df.label, test_df.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86504"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(test_df['label'] == test_df['predict'])/test_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jamespearce/repos/dl/data/sentiment/StackedEnsemble_AllModels_0_AutoML_20180806_222557.zip'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader.save_mojo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jamespearce/repos/dl/data/sentiment/StackedEnsemble_AllModels_0_AutoML_20180806_222557'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2o.save_model(aml.leader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_pickle(\"imdb_pred.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Doc2vec: https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "- Paper that inspired this: http://arxiv.org/abs/1405.4053"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
