{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dats versus cogs example\n",
    "\n",
    "This was taken from `fast.ai`.\n",
    "\n",
    "https://github.com/fastai/fastai/blob/master/courses/dl1/lesson1.ipynb\n",
    "\n",
    "## Image classification with Convolutional Neural Networks\n",
    "Welcome to the first week of the second deep learning certificate! We're going to use convolutional neural networks (CNNs) to allow our computer to see – something that is only possible thanks to deep learning.\n",
    "\n",
    "## The first task: 'Dogs vs Cats'\n",
    "\n",
    "We're going to try to create a model to enter the Dogs vs Cats competition at Kaggle. There are 25,000 labelled dog and cat photos available for training, and 12,500 in the test set that we have to try to label for this competition. According to the Kaggle web-site, when this competition was launched (end of 2013): \"State of the art: The current literature suggests machine classifiers can score above 80% accuracy on this task\". So if we can beat 80%, then we will be at the cutting edge as of 2013!\n",
    "\n",
    "> fast.ai from here: https://github.com/fastai/fastai/blob/master/courses/dl1/lesson1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "## commented some out for the time being\n",
    "#%reload_ext autoreload\n",
    "#%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from fast.ai\n",
    "\n",
    "from fastai.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.transforms import *\n",
    "from fastai.conv_learner import *\n",
    "from fastai.model import *\n",
    "from fastai.dataset import *\n",
    "from fastai.sgdr import *\n",
    "from fastai.plots import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data from Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sz` is the size that the images will be resized to in order to ensure that the training runs quickly. We'll be talking about this parameter a lot during the course. We will leave it at 224 for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/Users/jamespearce/repos/dl/data/dogscats/\"\n",
    "sz=224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to have NVidia CPUs available for `cuda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not on my MacBook! What about the MS DSVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, NVidia provides special accelerated functions for deep learning in a package called `CuDNN`. Although not strictly necessary, it will improve training performance significantly, and is included by default in all supported fastai configurations. Therefore, if the following does not return `True`, you may want to look into why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra steps if NOT using Crestle or Paperspace or our scripts\n",
    "\n",
    "The dataset is available at http://files.fast.ai/data/dogscats.zip. You can download it directly on your server by running the following line in your terminal. `wget http://files.fast.ai/data/dogscats.zip`. You should put the data in a subdirectory of this notebook's directory, called `data/`. Note that this data is already available in Crestle and the Paperspace fast.ai template.\n",
    "\n",
    "> I couldn't get this, so I downloaded direct from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pivot – using the DSVM\n",
    "\n",
    "Looking at the blog post of getting to second place in Kaggle in 22 minutes by Adrian Rosebrock.\n",
    "\n",
    "https://blogs.technet.microsoft.com/machinelearning/2018/02/22/22-minutes-to-2nd-place-in-a-kaggle-competition-with-deep-learning-azure/\n",
    "\n",
    "To get there, we use features from pre-trained Convolutional Neural Networks.\n",
    "\n",
    "Import the libraries we need, including `ResNet50` from `keras`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from keras.applications import ResNet50\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import progressbar\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical flow to classify an image using a CNN is this:\n",
    "\n",
    "  1. feed the image into the neural network\n",
    "  2. the image forward propagates through the neural network\n",
    "  3. the network outputs the final probabilities at the end\n",
    "  \n",
    "However, we **can** stop the propagation at an arbitrary layer, such as the activation or pooling layer, to give _feature vectors_ rather than probabilities.\n",
    "\n",
    "Where the network was trained on a rich data set of images, it gives extremely useful features that can be applied to other computer vision problems. We call this process **transfer learning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we are not using command line arguments (like we typically\n",
    "# would inside Deep Learning for Computer Vision with Python, let's\n",
    "# \"pretend\" we are by using an `args` dictionary -- this will enable\n",
    "# us to easily reuse and swap out code depending if we are using the\n",
    "# command line or Jupyter Notebook\n",
    "args = {\n",
    "    \"dataset\": \"/Users/jamespearce/repos/dl/data/dogscats/train\",\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "\n",
    "# store the batch size in a convenience variable\n",
    "bs = args[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "# grab the list of images in the Kaggle Dogs vs. Cats download and# grab  \n",
    "# shuffle them to allow for easy training and testing splits via\n",
    "# array slicing during training time\n",
    "imagePaths = list(paths.list_images(args[\"dataset\"]))\n",
    "random.shuffle(imagePaths)\n",
    "print(len(imagePaths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All files in the Dogs vs. Cats dataset have filenames such as `cat.153.jpg` or `dog.4375.jpg` – since the class labels are baked right into the filenames, we can easily extract them before the dot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the class labels from the image paths then encode the\n",
    "# labels\n",
    "labels = [p.split(os.path.sep)[-1].split(\".\")[0] for p in imagePaths]\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the `ResNet50` weights and load the model. This took around half an hour on a slow connection.\n",
    "\n",
    "Python would not download the weights on this connections, so downloaded separately from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5 and stored in `~/.keras/models`.\n",
    "\n",
    "In order to perform feature extraction, we need a pre-trained network – `ResNet50` is a good choice for this application. Notice how we have set `include_top=False` to leave off the fully-connected layers, enabling us to easily perform feature extraction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ResNet50 network (i.e., the network we'll be using for\n",
    "# feature extraction)\n",
    "\n",
    "model = ResNet50(weights=\"imagenet\", include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have all image paths we need to loop over them individually and build batches to pass through the network for feature extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:   0% |                                    | ETA:  --:--:--\r"
     ]
    }
   ],
   "source": [
    "# initialize the progress bar\n",
    "widgets = [\"Extracting Features: \", progressbar.Percentage(), \" \",\n",
    "    progressbar.Bar(), \" \", progressbar.ETA()]\n",
    "pbar = progressbar.ProgressBar(maxval=len(imagePaths),\n",
    "    widgets=widgets).start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = 0.1\n",
    "random.seed(2027)\n",
    "\n",
    "k = int(len(imagePaths) * 0.1)\n",
    "indices = random.sample(range(len(imagePaths)), k)\n",
    "\n",
    "imagePaths_sample = [imagePaths[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(1, 224, 224, 3)\n",
      "2\n",
      "(32, 7, 7, 2048)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 3211264 into shape (32,2048)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-fbfbdae425ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# a flattened feature vector of the `MaxPooling2D` outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# if our data matrix is None, initialize it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 3211264 into shape (32,2048)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# initialize our data matrix (where we will store our extracted\n",
    "# features)\n",
    "data = None\n",
    "\n",
    "# loop over the images in batches\n",
    "for i in np.arange(0, len(imagePaths_sample), bs):\n",
    "    # extract the batch of images and labels, then initialize the\n",
    "    # list of actual images that will be passed through the network\n",
    "    # for feature extraction\n",
    "    batchPaths = imagePaths_sample[i:i + bs]\n",
    "    batchLabels = labels[i:i + bs]\n",
    "    batchImages = []\n",
    "    \n",
    "    # loop over the images and labels in the current batch\n",
    "    for (j, imagePath) in enumerate(batchPaths):\n",
    "        # load the input image using the Keras helper utility\n",
    "        # while ensuring the image is resized to 224x224 pixels\n",
    "        image = load_img(imagePath, target_size=(224, 224))\n",
    "        image = img_to_array(image)\n",
    "\n",
    "        # preprocess the image by (1) expanding the dimensions and\n",
    "        # (2) subtracting the mean RGB pixel intensity from the\n",
    "        # ImageNet dataset\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        image = imagenet_utils.preprocess_input(image)\n",
    "\n",
    "        # add the image to the batch\n",
    "        batchImages.append(image)\n",
    "\n",
    "    # pass the images through the network and use the outputs as\n",
    "    # our actual features\n",
    "    batchImages = np.vstack(batchImages)\n",
    "\n",
    "    features = model.predict(batchImages, batch_size=bs)\n",
    "\n",
    "    # reshape the features so that each image is represented by\n",
    "    # a flattened feature vector of the `MaxPooling2D` outputs\n",
    "    print(features.shape)\n",
    "    features = features.reshape((features.shape[0], 2048))\n",
    "    \n",
    "    # if our data matrix is None, initialize it\n",
    "    if data is None:\n",
    "        data = features\n",
    "    \n",
    "    # otherwise, stack the data and features together\n",
    "    else:\n",
    "        data = np.vstack([data, features])\n",
    "    \n",
    "    # update the progress bar\n",
    "    pbar.update(i)\n",
    "\n",
    "# finish up the progress bar\n",
    "pbar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3211264"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7*7*32*2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
